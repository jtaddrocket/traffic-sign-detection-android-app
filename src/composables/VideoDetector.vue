<template>
  <ion-page class="page-transparent">
    <ion-content fullscreen class="content-transparent">
      <div id="cameraPreview" class="camera-preview"></div>
      <canvas ref="canvas" class="bounding-box-canvas"></canvas>

      <!-- Stack ·∫£nh ƒë·ªëi t∆∞·ª£ng ƒë√£ c·∫Øt -->
      <div class="detected-stack" v-if="snapshots.length">
        <div v-for="(snap, idx) in snapshots" :key="idx" class="detected-item">
          <img :src="snap.src" class="snap-img" />
          <p class="snap-label">{{ snap.label }}</p>
        </div>
      </div>
    </ion-content>
  </ion-page>
</template>

<script setup lang="ts">
console.log('[VD] VideoDetector.vue setup ch·∫°y');
import { ref, onMounted, onBeforeUnmount, nextTick } from 'vue';
import { CameraPreview, CameraPreviewOptions } from '@capacitor-community/camera-preview';
import * as tf from '@tensorflow/tfjs';
import { loadTFLiteModel, TFLiteModel } from '@tensorflow/tfjs-tflite';
import { TextToSpeech } from '@capacitor-community/text-to-speech';

console.log('üöÄ VideoDetector module loaded');

import rawMapping from '@/assets/label_mapping.json';
import vnMappingJson from '@/assets/label_vn_mapping.json';
const vnMapping = vnMappingJson as Record<string, string>;

const canvas = ref<HTMLCanvasElement | null>(null);
const displayLabel = ref<string | null>(null);

// Danh s√°ch ·∫£nh c·∫Øt ƒë·ªëi t∆∞·ª£ng
const snapshots = ref<{ src: string; label: string }[]>([]);

let intervalId: number;

const labelMapping = rawMapping as Record<string, string>;
let tfliteModel: TFLiteModel | null = null;
const MODEL_INPUT_WIDTH = 640;
const MODEL_INPUT_HEIGHT = 640;

let lastSpokenSeq = '';

// Ng∆∞·ª°ng confidence (t·ª´ 0 ƒë·∫øn 1)
const confThreshold = 0.5;

// C·∫•u h√¨nh smoothing: s·ªë frame ƒë·ªÉ gom vote nh√£n
const historySize = 5;
const predictionHistory: number[] = [];

// H√†m t√≠nh mode ‚Äì tr·∫£ v·ªÅ gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong m·∫£ng
function mode(arr: number[]): number {
  const freq = arr.reduce((acc, v) => {
    acc[v] = (acc[v] || 0) + 1;
    return acc;
  }, {} as Record<number, number>);
  return Number(
    Object.entries(freq)
      .sort(([, a], [, b]) => (b as number) - (a as number))[0][0]
  );
}

// ƒê·ªçc nhi·ªÅu nh√£n theo th·ª© t·ª± confidence (cao‚Üíth·∫•p)
async function speakLabels(labels: string[]) {
  const seq = labels.join('|');
  if (!labels.length || seq === lastSpokenSeq) return;
  lastSpokenSeq = seq;

  for (const lb of labels) {
    try {
      await TextToSpeech.speak({ text: lb, lang: 'vi-VN', rate: 1.0 });
    } catch (e) {
      console.warn('TTS error', e);
    }
  }
}

// speakLabel (ƒë∆°n) v·∫´n gi·ªØ ƒë·ªÉ d√πng l·∫°i n·∫øu c·∫ßn
async function speakLabel(label: string) {
  await speakLabels([label]);
}

async function initTFLiteModel() {
  console.log('‚öôÔ∏è initTFLiteModel() start');
  const url = import.meta.env.BASE_URL + 'assets/models/yolov8n_final_float32_nms.tflite';
  tfliteModel = await loadTFLiteModel(url);
  console.log('‚úÖ TFLite model ready');
  
  // Test model v·ªõi tensor zeros
  const testInput = tf.zeros([1, 640, 640, 3]);
  const testOutput = tfliteModel.predict(testInput);
  
  // Ki·ªÉm tra type c·ªßa testOutput
  if (testOutput instanceof tf.Tensor) {
    const tensor = testOutput as tf.Tensor;
    console.log('Test output shape:', tensor.shape);
    console.log('Test output sample:', await tensor.array());
  } else if (Array.isArray(testOutput)) {
    const firstTensor = testOutput[0] as tf.Tensor;
    console.log('Test output is array of tensors');
    console.log('First tensor shape:', firstTensor.shape);
    console.log('First tensor sample:', await firstTensor.array());
  } else {
    const outMap = testOutput as Record<string, tf.Tensor>;
    const key = outMap['Identity'] ? 'Identity' : Object.keys(outMap)[0];
    const tensor = outMap[key] as tf.Tensor;
    console.log('Test output is named tensor map');
    console.log('Output tensor shape:', tensor.shape);
    console.log('Output tensor sample:', await tensor.array());
  }
}

// ---
// Ti·ªÅn x·ª≠ l√Ω theo phong c√°ch "letterbox" ‚Äì gi·ªØ nguy√™n t·ªâ l·ªá, th√™m padding ƒë·ªÉ
// v·ª´a kh√≠t khung 640√ó640 gi·ªëng YOLO. H√†m tr·∫£ l·∫°i tensor ƒë√£ s·∫µn s√†ng cho model
// k√®m theo c√°c tham s·ªë ƒë·ªÉ map ng∆∞·ª£c bbox v·ªÅ ·∫£nh g·ªëc.
interface PreprocessResult {
  tensor: tf.Tensor4D;   // [1, 640, 640, 3]
  r: number;             // scale ratio (0‚Äí1]
  padX: number;          // padding ·ªü chi·ªÅu X (pixel, tr√™n input 640)
  padY: number;          // padding ·ªü chi·ªÅu Y (pixel)
  srcW: number;          // k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
  srcH: number;
}

function preprocessFrame(bitmap: ImageBitmap): PreprocessResult {
  // Chuy·ªÉn ·∫£nh th√†nh tensor [H, W, 3]
  const img = tf.browser.fromPixels(bitmap);
  const [srcH, srcW] = img.shape as number[];

  // T·ªâ l·ªá co
  const r = Math.min(MODEL_INPUT_WIDTH / srcW, MODEL_INPUT_HEIGHT / srcH);
  const newW = Math.round(srcW * r);
  const newH = Math.round(srcH * r);

  // Padding ƒë·ªÉ v·ª´a 640√ó640
  const padX = Math.floor((MODEL_INPUT_WIDTH - newW) / 2);
  const padY = Math.floor((MODEL_INPUT_HEIGHT - newH) / 2);

  // Resize + pad
  const processed = tf.tidy(() => {
    const resized = tf.image.resizeBilinear(img, [newH, newW]);
    const padded  = tf.pad(resized, [[padY, MODEL_INPUT_HEIGHT - newH - padY],
                                     [padX, MODEL_INPUT_WIDTH  - newW - padX],
                                     [0, 0]]);
    const normalized = padded.div(255);
    return normalized.expandDims(0) as tf.Tensor4D; // [1,640,640,3]
  });

  // img tensor ƒë√£ d√πng xong, gi·∫£i ph√≥ng
  tf.dispose(img);

  return { tensor: processed, r, padX, padY, srcW, srcH };
}

// 1) ƒê·ªãnh nghƒ©a interface cho box
interface Box {
  x1: number;  y1: number;
  x2: number;  y2: number;
  score: number;
  classId: number;
}

async function runInference(bitmap: ImageBitmap): Promise<Box[]> {
  if (!tfliteModel) {
    console.error('Model not loaded');
    return [];
  }

  // 1) Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o
  const { tensor, r, padX, padY, srcW, srcH } = preprocessFrame(bitmap);

  // 2) Th·ª±c hi·ªán suy lu·∫≠n
  const rawOutput = tfliteModel.predict(tensor);

  // C√°c bi·∫øn s·∫Ω ch·ª©a tensor ƒë·∫ßu ra tu·ª≥ thu·ªôc v√†o c·∫•u tr√∫c model
  let boxesTensor: tf.Tensor | undefined;
  let classesTensor: tf.Tensor | undefined;
  let scoresTensor: tf.Tensor | undefined;
  let countTensor: tf.Tensor | undefined;

  // a) Tr∆∞·ªùng h·ª£p ph·ªï bi·∫øn: model tr·∫£ v·ªÅ m·∫£ng 4 tensor
  if (Array.isArray(rawOutput) && rawOutput.length >= 4) {
    [boxesTensor, classesTensor, scoresTensor, countTensor] = rawOutput as tf.Tensor[];
  }
  // b) Tr∆∞·ªùng h·ª£p tr·∫£ v·ªÅ map t√™n-tensor
  else if (typeof rawOutput === 'object' && !(rawOutput instanceof tf.Tensor)) {
    const outMap = rawOutput as Record<string, tf.Tensor>;
    // S·∫Øp x·∫øp key ƒë·ªÉ l·∫•y theo th·ª© t·ª± ·ªïn ƒë·ªãnh (boxes, classes, scores, count)
    const keys = Object.keys(outMap).sort();
    boxesTensor   = outMap[keys[0]];
    classesTensor = outMap[keys[1]];
    scoresTensor  = outMap[keys[2]];
    countTensor   = outMap[keys[3]];
  }
  // c) Tr∆∞·ªùng h·ª£p cu·ªëi: 1 tensor, shape [1, N, 6] (x1, y1, x2, y2, score, class)
  else if (rawOutput instanceof tf.Tensor) {
    boxesTensor = rawOutput;
  }

  const boxes: Box[] = [];

  // ---- Gi·∫£i m√£ output khi ƒë√£ c√≥ c√°c tensor ri√™ng bi·ªát ----
  if (boxesTensor && classesTensor && scoresTensor) {
    const [boxesArr, classesArr, scoresArr, countArr] = await Promise.all([
      boxesTensor.array() as Promise<number[][][]>,
      classesTensor.array() as Promise<number[][]>,
      scoresTensor.array() as Promise<number[][]>,
      countTensor ? countTensor.array() as Promise<number[]> : Promise.resolve([ (boxesTensor.shape[1] || 0) ])
    ]);

    const numDetections = Math.min(countArr[0] ?? boxesArr[0].length, boxesArr[0].length);

    for (let i = 0; i < numDetections; i++) {
      const score = scoresArr[0][i];
      if (score < confThreshold) continue;

      // TFLite Detection PostProcess xu·∫•t [ymin, xmin, ymax, xmax]
      const [ymin, xmin, ymax, xmax] = boxesArr[0][i];
      const cls = Math.round(classesArr[0][i]);

      const x1Pix = ((xmin * MODEL_INPUT_WIDTH)  - padX) / r;
      const y1Pix = ((ymin * MODEL_INPUT_HEIGHT) - padY) / r;
      const x2Pix = ((xmax * MODEL_INPUT_WIDTH)  - padX) / r;
      const y2Pix = ((ymax * MODEL_INPUT_HEIGHT) - padY) / r;

      // Chu·∫©n ho√° v·ªÅ 0‚Äí1 theo k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
      const x1 = Math.max(0, Math.min(1, x1Pix / srcW));
      const y1 = Math.max(0, Math.min(1, y1Pix / srcH));
      const x2 = Math.max(0, Math.min(1, x2Pix / srcW));
      const y2 = Math.max(0, Math.min(1, y2Pix / srcH));

      boxes.push({
        x1,
        y1,
        x2,
        y2,
        score,
        classId: cls,
      });
    }

    tf.dispose([tensor, boxesTensor, classesTensor, scoresTensor]);
    if (countTensor) tf.dispose(countTensor);
  }
  // ---- Gi·∫£i m√£ output khi model cho 1 tensor [1, N, 6] ----
  else if (boxesTensor) {
    const arr = await boxesTensor.array() as number[][][]; // [1, N, 6]
    const data = arr[0];

    for (let i = 0; i < data.length; i++) {
      const [rawX1, rawY1, rawX2, rawY2, score, clsFloat] = data[i] as unknown as number[];
      if (score < confThreshold) continue;
      const cls = Math.round(clsFloat);

      const x1Pix = ((rawX1 * MODEL_INPUT_WIDTH)  - padX) / r;
      const y1Pix = ((rawY1 * MODEL_INPUT_HEIGHT) - padY) / r;
      const x2Pix = ((rawX2 * MODEL_INPUT_WIDTH)  - padX) / r;
      const y2Pix = ((rawY2 * MODEL_INPUT_HEIGHT) - padY) / r;

      // Chu·∫©n ho√° v·ªÅ 0‚Äí1 theo k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
      const x1 = Math.max(0, Math.min(1, x1Pix / srcW));
      const y1 = Math.max(0, Math.min(1, y1Pix / srcH));
      const x2 = Math.max(0, Math.min(1, x2Pix / srcW));
      const y2 = Math.max(0, Math.min(1, y2Pix / srcH));

      boxes.push({ x1, y1, x2, y2, score, classId: cls });
    }

    tf.dispose([tensor, boxesTensor]);
  }

  // S·∫Øp x·∫øp ƒë·ªÉ bounding-box c√≥ score cao nh·∫•t ƒë·ª©ng ƒë·∫ßu
  boxes.sort((a, b) => b.score - a.score);

  return boxes;
}

function drawAndSpeak(boxes: Box[], bitmap: ImageBitmap, videoW: number, videoH: number) {
  const cv = canvas.value!;
  const ctx = cv.getContext('2d')!;
  ctx.clearRect(0, 0, cv.width, cv.height);

  if (!boxes.length) {
    displayLabel.value = null;
    snapshots.value = [];
    return;
  }

  // Reset snapshots m·ªói khung h√¨nh
  snapshots.value = [];

  // V·∫Ω t·∫•t c·∫£ c√°c bounding-box
  boxes.forEach(b => {
    const x = b.x1 * videoW;
    const y = b.y1 * videoH;
    const w = (b.x2 - b.x1) * videoW;
    const h = (b.y2 - b.y1) * videoH;
    
    ctx.strokeStyle = 'red';
    ctx.lineWidth   = 2;
    ctx.strokeRect(x, y, w, h);

    const className = labelMapping[b.classId.toString()] || b.classId.toString();
    // Hi·ªÉn th·ªã m√£ nh√£n (className) tr√™n bounding box
    ctx.font      = '16px sans-serif';
    ctx.fillStyle = 'red';
    ctx.fillText(
      className + ' (' + Math.round(b.score * 100) + '%)', 
      x,
      y > 20 ? y - 5 : y + 20
    );

    // --- C·∫Øt v√† l∆∞u ·∫£nh ƒë·ªëi t∆∞·ª£ng ---
    try {
      // To·∫° ƒë·ªô theo bitmap g·ªëc
      const sx = Math.max(0, Math.round(b.x1 * bitmap.width));
      const sy = Math.max(0, Math.round(b.y1 * bitmap.height));
      const sw = Math.max(1, Math.round((b.x2 - b.x1) * bitmap.width));
      const sh = Math.max(1, Math.round((b.y2 - b.y1) * bitmap.height));

      // Canvas t·∫°m ƒë·ªÉ crop & downscale (128px max)
      const maxDim = 128;
      const scale   = Math.min(1, maxDim / Math.max(sw, sh));
      const dw = Math.round(sw * scale);
      const dh = Math.round(sh * scale);

      const tmpCanvas = document.createElement('canvas');
      tmpCanvas.width  = dw;
      tmpCanvas.height = dh;

      const tmpCtx = tmpCanvas.getContext('2d')!;
      tmpCtx.drawImage(bitmap, sx, sy, sw, sh, 0, 0, dw, dh);
      const dataUrl = tmpCanvas.toDataURL('image/jpeg', 0.8);

      snapshots.value.push({ src: dataUrl, label: vnMapping[className] || className });
    } catch (e) {
      console.warn('Crop error', e);
    }
  });

  // --- Chu·∫©n b·ªã danh s√°ch nh√£n theo th·ª© t·ª± confidence cao ‚Üí th·∫•p ---
  const spokenLabels: string[] = [];
  boxes.forEach(b => {
    const nameEn = labelMapping[b.classId.toString()] || b.classId.toString();
    const vnLabel = vnMapping[nameEn] || nameEn;
    if (!spokenLabels.includes(vnLabel)) spokenLabels.push(vnLabel);
  });

  if (spokenLabels.length) {
    // N·∫øu ‚â•2 ƒë·ªëi t∆∞·ª£ng ƒë·ªçc l·∫ßn l∆∞·ª£t, ng∆∞·ª£c l·∫°i ƒë·ªçc 1 ƒë·ªëi t∆∞·ª£ng
    speakLabels(spokenLabels);
  }
}

async function captureFrame(): Promise<ImageBitmap | null> {
  try {
    // L·∫•y frame ƒë√£ down-scale, m·∫∑c ƒë·ªãnh ~previewSize
    const { value: b64 } = await CameraPreview.captureSample({ quality: 60 });

    const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));
    return await createImageBitmap(new Blob([bytes], { type: 'image/jpeg' }));
  } catch (e) {
    console.error('captureFrame error', e);
    return null;
  }
}

let isProcessing = false;

async function processLoop() {
  if (isProcessing || !tfliteModel) return;
  isProcessing = true;

  const bitmap = await captureFrame();
  if (!bitmap) {
    isProcessing = false;
    return;
  }

  const cv = canvas.value;
  if (!cv) {
    bitmap.close();
    isProcessing = false;
    return;
  }

  try {
    const dets = await runInference(bitmap);
    if (dets.length) {
      drawAndSpeak(dets, bitmap, cv.width, cv.height);
    } else {
      // Kh√¥ng c√≥ detections: x√≥a canvas v√† nh√£n
      const ctx = cv.getContext('2d')!;
      ctx.clearRect(0, 0, cv.width, cv.height);
      displayLabel.value = null;
    }
  } catch (e) {
    console.error('Inference error', e);
  } finally {
    bitmap.close();
    isProcessing = false;
  }
}

async function startCamera() {
  // 1) ƒê·∫£m b·∫£o camera c≈© d·ª´ng
  try { await CameraPreview.stop() } catch {}

  // 2) ƒê·ª£i DOM render xong
  await nextTick()

  // 3) L·∫•y k√≠ch th∆∞·ªõc th·ª±c c·ªßa div#cameraPreview
  const previewEl = document.getElementById('cameraPreview')
  const vw = previewEl?.clientWidth  || window.innerWidth
  const vh = previewEl?.clientHeight || window.innerHeight
  console.log('‚ÑπÔ∏è cameraPreview size:', vw, 'x', vh)

  // 4) C·∫•u h√¨nh v√† start
  const opts: CameraPreviewOptions = {
    parent: 'cameraPreview',
    width:  vw,
    height: vh,
    position: 'rear',
    toBack: true,
  }

  try {
    await CameraPreview.start(opts)
    console.log('‚úÖ CameraPreview.start() ok')
  } catch (e: any) {
    console.error('‚ùå Kh√¥ng th·ªÉ start camera:', e)
    return
  }

  // 5) Thi·∫øt l·∫≠p canvas c≈©ng b·∫±ng k√≠ch th∆∞·ªõc n√†y
  if (canvas.value) {
    canvas.value.width  = vw
    canvas.value.height = vh
  }

  // 6) B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p inference
  if (intervalId) clearInterval(intervalId)
  intervalId = window.setInterval(processLoop, 250)
}


async function stopCamera() {
  clearInterval(intervalId);
  try {
    await CameraPreview.stop();
  } catch (e) {
    console.warn('L·ªói khi stop camera:', e);
  }
}

onMounted(async () => {
  console.log('üîî onMounted() fired')

  // 1) ƒê·ª£i TF.js kh·ªüi xong
  console.log('üß† waiting tf.ready()...')
  await tf.ready()
  console.log('üß† tf.ready() done')

  try {
    // 2) Kh·ªüi t·∫°o TFLite v√† camera
    await initTFLiteModel()
    await startCamera()
  } catch (e) {
    console.error('Init error', e)
  }
})

onBeforeUnmount(async () => {
  clearInterval(intervalId)
  try {
    await CameraPreview.stop()
  } catch {}
})
</script>

<style scoped>
.page-transparent {
  --background: transparent !important;
}

.content-transparent {
  --background: transparent !important;
}

.camera-preview {
  position: absolute;
  top: 0;
  left: 0;
  width: 100vw;
  height: 100vh;
  z-index: 0;
}

.bounding-box-canvas {
  position: absolute;
  top: 0;
  left: 0;
  width: 100vw;
  height: 100vh;
  pointer-events: none;
  z-index: 10;
}

.detected-stack {
  position: absolute;
  left: 10px;
  bottom: 10px;
  display: flex;
  flex-direction: row;
  align-items: center;
  gap: 8px;
  z-index: 30;
}

.detected-item {
  background: rgba(0,0,0,0.6);
  padding: 4px;
  border-radius: 4px;
  text-align: center;
}

.snap-img {
  max-width: 128px;
  max-height: 128px;
  display: block;
}

.snap-label {
  color: white;
  font-size: 12px;
  margin: 2px 0 0 0;
}
</style>